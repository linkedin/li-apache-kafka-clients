/*
 * Copyright 2019 LinkedIn Corp. Licensed under the BSD 2-Clause License (the "License").â€¨ See License in the project root for license information.
 */

package com.linkedin.kafka.clients.consumer;

import com.linkedin.kafka.clients.common.ClusterDescriptor;
import com.linkedin.kafka.clients.common.ClusterGroupDescriptor;
import com.linkedin.kafka.clients.common.LiKafkaFederatedClient;
import com.linkedin.kafka.clients.common.LiKafkaFederatedClientType;
import com.linkedin.kafka.clients.metadataservice.MetadataServiceClient;
import com.linkedin.kafka.clients.metadataservice.MetadataServiceClientException;
import com.linkedin.kafka.clients.utils.LiKafkaClientsUtils;
import com.linkedin.mario.common.websockets.MsgType;

import java.time.Duration;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.Set;
import java.util.UUID;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.regex.Pattern;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.clients.consumer.OffsetAndTimestamp;
import org.apache.kafka.clients.consumer.OffsetCommitCallback;
import org.apache.kafka.common.KafkaException;
import org.apache.kafka.common.Metric;
import org.apache.kafka.common.MetricName;
import org.apache.kafka.common.PartitionInfo;
import org.apache.kafka.common.TopicPartition;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


/**
 * This is a consumer implementation that works with a federated Kafka cluster, which consists of one or more physical
 * Kafka clusters. This class is not thread-safe.
 */
public class LiKafkaFederatedConsumerImpl<K, V> implements LiKafkaConsumer<K, V>, LiKafkaFederatedClient {
  private static final Logger LOG = LoggerFactory.getLogger(LiKafkaFederatedConsumerImpl.class);
  private static final Duration CONSUMER_CLOSE_MAX_TIMEOUT = Duration.ofMinutes(10);
  private static final Duration RELOAD_CONFIG_EXECUTION_TIMEOUT = Duration.ofMinutes(10);
  private static final AtomicInteger FEDERATED_CONSUMER_CLIENT_ID_SEQUENCE = new AtomicInteger(1);

  // The cluster group this client is talking to
  private ClusterGroupDescriptor _clusterGroup;

  // The default timeout for blocking calls specified by default.api.timeout.ms consumer properties.
  // This is exposed for federated consumer methods with the default timeout that need to invoke the individual
  // consumer's corresponding methods concurrently.
  private int _defaultApiTimeoutMs;

  // The client for the metadata service which serves cluster and topic metadata
  private MetadataServiceClient _mdsClient;

  // Timeout in milliseconds for metadata service requests.
  private int _mdsRequestTimeoutMs;

  private class ClusterConsumerPair<K, V> {
    private final ClusterDescriptor _cluster;
    private final LiKafkaConsumer<K, V> _consumer;

    public ClusterConsumerPair(ClusterDescriptor cluster, LiKafkaConsumer<K, V> consumer) {
      _cluster = cluster;
      _consumer = consumer;
    }

    public ClusterDescriptor getCluster() {
      return _cluster;
    }

    public LiKafkaConsumer<K, V> getConsumer() {
      return _consumer;
    }
  }

  // Per cluster Consumers
  private volatile List<ClusterConsumerPair<K, V>> _consumers;

  // Consumer builder for creating per-cluster LiKafkaConsumer
  private LiKafkaConsumerBuilder<K, V> _consumerBuilder;

  // Consumer configs common to all clusters
  private LiKafkaConsumerConfig _commonConsumerConfigs;

  // max.poll.records for the federated consumer
  private int _maxPollRecordsForFederatedConsumer;

  // metadata.max.age.ms, also used as the interval for polling the creation of nonexistent topics that are part of the
  // current assignment/subscription
  private int _metadataMaxAgeMs;

  // The number of clusters in this cluster group to connect to for the current assignment/subscription
  private int _numClustersToConnectTo;

  private int _nextClusterIndexToPoll;

  // The prefix of the client.id property to be used for individual consumers. Since the client id is part of the
  // consumer metric keys, we need to use cluster-specific client ids to differentiate metrics from different clusters.
  // Per-cluster client id is generated by appending the cluster name to this prefix.
  //
  // If user set the client.id property for the federated client, it will be used as the prefix. If not, a new prefix
  // is generated.
  private String _clientIdPrefix;

  private volatile boolean _closed;

  // Number of config reload operations executed
  private volatile int _numConfigReloads;

  // This contains the result of the location lookup for a set of topic partitions across multiple clusters in a cluster
  // group.
  private class PartitionsByClusterResult {
    private Map<ClusterDescriptor, Collection<TopicPartition>> _clusterToPartitionsMap;
    private Set<String> _nonexistentTopics;

    public PartitionsByClusterResult() {
      _clusterToPartitionsMap = Collections.emptyMap();
      _nonexistentTopics = Collections.emptySet();
    }

    public PartitionsByClusterResult(Map<ClusterDescriptor, Collection<TopicPartition>> clusterToPartitionsMap,
        Set<String> nonexistentTopics) {
      _clusterToPartitionsMap = clusterToPartitionsMap;
      _nonexistentTopics = nonexistentTopics;
    }

    public Map<ClusterDescriptor, Collection<TopicPartition>> getClusterToPartitionsMap() {
      return _clusterToPartitionsMap;
    }

    public Set<String> getNonexistentTopics() {
      return _nonexistentTopics;
    }
  }

  // Current assignment/subscription state at the federated level
  private FederatedSubscriptionState _currentSubscription;

  // Callback interface for executing methods that need to be called with cluster-specific input values and timeout.
  interface ExecutorCallback<K, V, I> {
    void execute(LiKafkaConsumer<K, V> consumer, I perClusterInput, Duration timeout);
  }

  public LiKafkaFederatedConsumerImpl(Properties props) {
    this(new LiKafkaConsumerConfig(props), null, null);
  }

  public LiKafkaFederatedConsumerImpl(Properties props, MetadataServiceClient mdsClient, LiKafkaConsumerBuilder<K, V> consumerBuilder) {
    this(new LiKafkaConsumerConfig(props), mdsClient, consumerBuilder);
  }

  public LiKafkaFederatedConsumerImpl(Map<String, ?> configs) {
    this(new LiKafkaConsumerConfig(configs), null, null);
  }

  public LiKafkaFederatedConsumerImpl(Map<String, ?> configs, MetadataServiceClient mdsClient,
      LiKafkaConsumerBuilder<K, V> consumerBuilder) {
    this(new LiKafkaConsumerConfig(configs), mdsClient, consumerBuilder);
  }

  @SuppressWarnings("unchecked")
  private LiKafkaFederatedConsumerImpl(LiKafkaConsumerConfig configs, MetadataServiceClient mdsClient,
      LiKafkaConsumerBuilder<K, V> consumerBuilder) {
    _commonConsumerConfigs = configs;
    _clusterGroup = new ClusterGroupDescriptor(configs.getString(LiKafkaConsumerConfig.CLUSTER_GROUP_CONFIG),
        configs.getString(LiKafkaConsumerConfig.CLUSTER_ENVIRONMENT_CONFIG));

    // Each per-cluster consumer and auditor will be instantiated by the passed-in consumer builder when the client
    // begins to consume from that cluster. If a null builder is passed, create a default one, which builds
    // LiKafkaConsumer.

    _consumers = new ArrayList<>();
    _consumerBuilder = consumerBuilder != null ? consumerBuilder : new LiKafkaConsumerBuilder<K, V>();

    _defaultApiTimeoutMs = configs.getInt(ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);
    _mdsRequestTimeoutMs = configs.getInt(LiKafkaConsumerConfig.METADATA_SERVICE_REQUEST_TIMEOUT_MS_CONFIG);
    _maxPollRecordsForFederatedConsumer = configs.getInt(LiKafkaConsumerConfig.MAX_POLL_RECORDS_CONFIG);
    _metadataMaxAgeMs = configs.getInt(ConsumerConfig.METADATA_MAX_AGE_CONFIG);
    _currentSubscription = Unsubscribed.getInstance();
    _nextClusterIndexToPoll = 0;

    String clientIdPrefix = (String) configs.originals().get(ConsumerConfig.CLIENT_ID_CONFIG);
    if (clientIdPrefix == null || clientIdPrefix.length() == 0) {
      clientIdPrefix = "consumer-" + FEDERATED_CONSUMER_CLIENT_ID_SEQUENCE.getAndIncrement();
    }
    _clientIdPrefix = clientIdPrefix;

    _closed = false;
    _numConfigReloads = 0;

    try {
      // Instantiate metadata service client if necessary.
      _mdsClient = mdsClient != null ? mdsClient :
          configs.getConfiguredInstance(LiKafkaConsumerConfig.METADATA_SERVICE_CLIENT_CLASS_CONFIG, MetadataServiceClient.class);

      // Register this federated client with the metadata service. The metadata service will assign a UUID to this
      // client, which will be used for later interaction between the metadata service and the client.
      //
      // Registration may also return further information such as the metadata server version and any protocol settings.
      // We assume that such information will be kept and used by the metadata service client itself.
      //
      // TODO: make sure this is not blocking indefinitely and also works when Mario is not available.
      _mdsClient.registerFederatedClient(this, _clusterGroup, configs.originals(), _mdsRequestTimeoutMs);
    } catch (Exception e) {
      try {
        if (_mdsClient != null) {
          _mdsClient.close(_mdsRequestTimeoutMs);
        }
      } catch (Exception e2) {
        e.addSuppressed(e2);
      }
      throw e;
    }

    // Create a watchdog thread that polls the creation of nonexistent topics in the current assignment/subscription
    // and re-assign/subscribe if any of them have been created since the last poll.
    ScheduledExecutorService es = Executors.newSingleThreadScheduledExecutor();
    Thread t = new Thread(() -> {
      refreshSubscriptionMetadata();
    });
    t.setDaemon(true);
    t.setName("LiKafkaConsumer-refresh-subscription-metadata");
    t.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
      public void uncaughtException(Thread t, Throwable e) {
        throw new KafkaException("Thread " + t.getName() + " throws exception", e);
      }
    });
    es.scheduleAtFixedRate(t, 0, _metadataMaxAgeMs, TimeUnit.MILLISECONDS);
  }

  @Override
  public LiKafkaFederatedClientType getClientType() {
    return LiKafkaFederatedClientType.FEDERATED_CONSUMER;
  }

  /**
   * Get the set of partitions currently assigned to this consumer. If subscription happened by directly assigning
   * partitions using {@link #assign(Collection)} then this will simply return the same partitions that
   * were assigned. If topic subscription was used, then this will give the set of topic partitions currently assigned
   * to the consumer (which may be none if the assignment hasn't happened yet, or the partitions are in the
   * process of getting reassigned).
   * @return The set of partitions currently assigned to this consumer
   */
  @Override
  synchronized public Set<TopicPartition> assignment() {
    ensureOpen();
    switch (_currentSubscription.getSubscriptionType()) {
      case SubscriptionType.USER_ASSIGNED:
        return ((UserAssigned) _currentSubscription).getAssignment();

      case SubscriptionType.AUTO_TOPIC:
      case SubscriptionType.AUTO_PATTERN: {
        // Get the aggregate of currently assigned partitions from all consumers.
        Set<TopicPartition> aggregate = new HashSet<>();
        for (ClusterConsumerPair<K, V> entry : getImmutableConsumerList()) {
          aggregate.addAll(entry.getConsumer().assignment());
        }
        return aggregate;
      }

      case SubscriptionType.NONE:
        return Collections.emptySet();

      default:
        throw IllegalStateException("invalid subscription type: " + _currentSubscription.getSubscriptionType().name());
    }
  }

  @Override
  synchronized public Set<String> subscription() {
    throw new UnsupportedOperationException("Not implemented yet");
  }

  @Override
  synchronized public void subscribe(Collection<String> topics) {
    throw new UnsupportedOperationException("Not implemented yet");
  }

  @Override
  synchronized public void subscribe(Collection<String> topics, ConsumerRebalanceListener callback) {
    throw new UnsupportedOperationException("Not implemented yet");
  }

  private PartitionsByClusterResult getPartitionsByCluster(Set<TopicPartition> partitions) {
    ensureOpen();
    if (partitions == null) {
      throw new IllegalArgumentException("partitions cannot be null");
    }

    if (partitions.isEmpty()) {
      return new PartitionsByClusterResult();
    }

    Map<TopicPartition, ClusterDescriptor> partitionToClusterMap;
    try {
      partitionToClusterMap =
          _mdsClient.getClustersForTopicPartitions(partitions, _clusterGroup, _mdsRequestTimeoutMs);
    } catch (MetadataServiceClientException e) {
      throw new KafkaException("failed to get clusters for topic partitions " + partitions + ": ", e);
    }

    // Reverse the map so that we can have per-cluster topic partition sets.
    Map<ClusterDescriptor, Collection<TopicPartition>> clusterToPartitionsMap = new HashMap<>();
    Set<String> nonexistentTopics = new HashSet<>();
    for (Map.Entry<TopicPartition, ClusterDescriptor> entry : partitionToClusterMap.entrySet()) {
      TopicPartition topicPartition = entry.getKey();
      ClusterDescriptor cluster = entry.getValue();
      if (cluster == null) {
        nonexistentTopics.add(topicPartition.topic());
      } else {
        clusterToPartitionsMap.computeIfAbsent(cluster, k -> new HashSet<TopicPartition>()).add(topicPartition);
      }
    }
    return new PartitionsByClusterResult(clusterToPartitionsMap, nonexistentTopics);
  }

  /**
   * Manually assign a list of partitions to this consumer. This interface does not allow for incremental assignment
   * and will replace the previous assignment (if there is one).
   * <p>
   * If the given list of topic partitions is empty, it is treated the same as {@link #unsubscribe()}.
   * <p>
   * Manual topic assignment through this method does not use the consumer's group management
   * functionality. As such, there will be no rebalance operation triggered when group membership or cluster and topic
   * metadata change. Note that it is not possible to use both manual partition assignment with {@link #assign(Collection)}
   * and group assignment with {@link #subscribe(Collection, ConsumerRebalanceListener)}.
   * <p>
   * If auto-commit is enabled, an async commit (based on the old assignment) will be triggered before the new
   * assignment replaces the old one.
   *
   * @param partitions The list of partitions to assign this consumer
   * @throws IllegalArgumentException If partitions is null or contains null or empty topics
   * @throws IllegalStateException If {@code subscribe()} is called previously with topics or pattern
   *                               (without a subsequent call to {@link #unsubscribe()})
   */
  @Override
  synchronized public void assign(Collection<TopicPartition> partitions) {
    ensureOpen();
    if (partitions == null) {
      throw new IllegalArgumentException("Topic partition collection to assign to cannot be null");
    }

    // Since the partitions to be assigned may belong to a differet subset of clusters for the previous
    // assignment/subscription, unsubscribe first to start with a clean slate.
    unsubscribe();

    // If the give list of partitions is empty, we are done here.
    if (partitions.isEmpty()) {
      return;
    }

    // TODO: May throw an exception if there exists a partition whose topic exists but the specified partition number is
    // out of range for that topic. Whether of not to throw an exception can be controlled by a property.
    // TODO: Can we use Collection instead of Set?
    Set<TopicPartition> partitionsSet = new HashSet<>(partitions);
    PartitionsByClusterResult partitionsByClusterResult = getPartitionsByCluster(partitionsSet);

    _currentSubscription = new UserAssigned(partitionsSet, partitionsByClusterResult.getNonexistentTopics());
    if (!partitionsByClusterResult.getNonexistentTopics().isEmpty()) {
      LOG.warn("the following topics in the requested assignment currently do not exist: {}",
          partitionsByClusterResult.getNonexistentTopics());
    }

    Map<ClusterDescriptor, Collection<TopicPartition>> clusterToPartitionsMap =
        partitionsByClusterResult.getClusterToPartitionsMap();
    _numClustersToConnectTo = clusterToPartitionsMap.size();
    _nextClusterIndexToPoll = 0;

    List<ClusterConsumerPair<K, V>> consumers = new ArrayList<>();
    for (Map.Entry<ClusterDescriptor, Collection<TopicPartition>> entry : clusterToPartitionsMap.entrySet()) {
      LiKafkaConsumer<K, V> curConsumer = createPerClusterConsumer(entry.getKey());
      curConsumer.assign(entry.getValue());
      consumers.add(new ClusterConsumerPair<K, V>(cluster, curConsumer));
    }
    _consumers = consumers;
  }

  @Override
  synchronized public void subscribe(Pattern pattern, ConsumerRebalanceListener callback) {
    throw new UnsupportedOperationException("Not implemented yet");
  }

  @Override
  synchronized public void subscribe(Pattern pattern) {
    throw new UnsupportedOperationException("Not implemented yet");
  }

  @Override
  synchronized public void unsubscribe() {
    ensureOpen();
    _currentSubscription = Unsubscribed.getInstance();
    if (_consumers.isEmpty()) {
      return;
    }
    for (ClusterConsumerPair<K, V> entry : _consumers) {
      LiKafkaConsumer<K, V> consumer = entry.getConsumer();
      consumer.unsubscribe();
    }
    // TODO: is it okay to not close? Close may take some time.
    closeNoLock(_defaultApiTimeoutMs);
    _consumers.clear();
  }

  @Override
  synchronized public ConsumerRecords<K, V> poll(long timeout) {
    ensureOpen();
    // Get an immutable copy of the current set of consumers.
    List<ClusterConsumerPair<K, V>> consumers = getImmutableConsumerList();

    if (consumers.isEmpty()) {
      return ConsumerRecords.empty();
    }

    long deadlineMs = System.currentTimeMillis() + timeout;
    Map<TopicPartition, List<ConsumerRecord<K, V>>> aggregatedConsumerRecords = new HashMap<>();
    Map<TopicPartition, ClusterDescriptor> partitionToClusterMap = new HashMap<>();
    int numRecordsReceived = 0;
    int currentClusterIndex = _nextClusterIndexToPoll;
    for (int i = 0; i < consumers.size(); i++) {
      ClusterConsumerPair<K, V> entry = consumers.get(currentClusterIndex);
      ClusterDescriptor cluster = entry.getCluster();
      LiKafkaConsumer<K, V> consumer = entry.getConsumer();

      // For the last consumer in the list, poll with the remaining timeout. Otherwise, poll with a zero timeout.
      ConsumerRecords<K, V> pollResult;
      if (i == consumers.size() - 1) {
        pollResult = consumer.poll(Math.max(deadlineMs - System.currentTimeMillis(), 0));
      } else {
        pollResult = consumer.poll(0);
      }

      for (TopicPartition partition : pollResult.partitions()) {
        // There should be no overlap between topic partitions returned from different clusters.
        if (aggregatedConsumerRecords.containsKey(partition)) {
          throw new IllegalStateException(
              "Duplicate topic partition " + partition + " exists in clusters " +
                  cluster + " and " + partitionToClusterMap.get(partition).getName() + " in group " + _clusterGroup);
        }
        aggregatedConsumerRecords.put(partition, pollResult.records(partition));
        partitionToClusterMap.put(partition, cluster);
      }
      numRecordsReceived += pollResult.count();
      currentClusterIndex = (currentClusterIndex + 1) % consumers.size();

      // If we have received the maximum number of records allowed for this federated consumer (in case where the number
      // of clusters in the group >= max.poll.records for the federated consumer) or the timeout has already passed
      // before finishing the iteration, stop here.
      if (numRecordsReceived == _maxPollRecordsForFederatedConsumer || System.currentTimeMillis() > deadlineMs) {
        break;
      }
    }

    _nextClusterIndexToPoll = (_nextClusterIndexToPoll + 1) % consumers.size();

    return new ConsumerRecords<K, V>(aggregatedConsumerRecords);
  }

  @Override
  public ConsumerRecords<K, V> poll(Duration timeout) {
    return poll(timeout.toMillis());
  }

  @Override
  synchronized public void commitSync() {
    commitSync(Duration.ofMillis(_defaultApiTimeoutMs));
  }

  @Override
  synchronized public void commitSync(Duration timeout) {
    executeCallbackWithTimeout("commitSync", null,
        (consumer, ignored, timeoutDuration) -> consumer.commitSync(timeoutDuration), timeout);
  }

  @Override
  public void commitSync(Map<TopicPartition, OffsetAndMetadata> offsets) {
    commitSync(offsets, Duration.ofMillis(_defaultApiTimeoutMs));
  }

  // ATTN: UnknownTopicOrPartitionException may be received - this is a retriable exception..
  // if not resolved by the time, timeout exception
  @Override
  synchronized public void commitSync(Map<TopicPartition, OffsetAndMetadata> offsets, Duration timeout) {
    long deadlineTimeMs = System.currentTimeMillis() + timeout.toMillis();
    while (System.currentTimeMillis() < deadlineTimeMs) {
      long startTimeMs = System.currentTimeMillis();
      PartitionValueMapByClusterResult offsetsByClusterResult = getPartitionValueMapByCluster(offsets);
      // Commit offset with existing partitions first.
      executeCallbackWithTimeout("commitSync", offsetsByClusterResult.getPartitionsValueMapByCluster(),
          (consumer, perClusterOffsets, timeoutDuration) -> {
        consumer.commitSync((Map<TopicPartition, OffsetAndMetadata>) perClusterOffsets, timeoutDuration);
      }, Duration.ofMillis(deadlineTimeMs - System.currentTimeMillis()));

      // Vanilla Kafka consumer retries on nonexistent partitions after waiting retry.backoff.ms until the given timeout
      // is reached. Mimic this behavior.
      // ATTN: This is slightly different behavior since the above call also retries.. but there is no good way.
      // TODO: update when the underlying metadata service can detect nonexistent partitions of existing topics, not
      // just nonexistent topics.
      if (offsetsByClusterResult.getNonexistentTopics().isEmpty()) {
        return;
      }

      // Sleep and retry;
      Thread.sleep(Math.Min(startTimeMs.deadlineTimeMs - System.currentTimeMillis(), _metadataMaxAgeMs));
    }
    throw new TimeoutException();
  }

  // A wrapper callback for async offset commits, which will call the user-provided callback only after all per-cluster
  // consumers finish commit and call this callback. If multiple consumers encounter exceptions while commiting offsets,
  // one of them will be passed to the user-provided callback.
  private class WrapperOffsetCommitCallback implements OffsetCommitCallback {
    private AtomicInteger _pendingCommitCount;
    // An aggregate of all offsets passed to each callback
    private Map<TopicPartition, OffsetAndMetadata> _allOffsets;
    private volatile Exception _exception;
    private OffsetCommitCallback _userCallback;

    public WrapperOffsetCommitCallback(AtomicInteger pendingCommitCount,
        Map<TopicPartition, OffsetAndMetadata> allOffsets, Exception exception, OffsetCommitCallback userCallback) {
      _pendingCommitCount = pendingCommitCount;
      _allOffsets = allOffsets;
      _exception = exception;
      _userCallback = userCallback;
    }

    @Override
    public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {
      synchronized (_allOffsets) {
        _allOffsets.putAll(offsets);
      }
      if (exception != null) {
        _exception = exception;
      }
      if (_pendingCommitCount.decrementAndGet() == 0) {
        if (_userCallback == null) {
          // Copies what DefaultOffsetCommitCallback does, which is used by upstream consumer when a null callback is
          // passed.
          if (_exception != null) {
              LOG.error("Offset commit with offsets {} failed", _allOffsets, _exception);
          }
        } else {
          _userCallback.onComplete(_allOffsets, _exception);
        }
      }
    }
  }

  @Override
  public void commitAsync() {
    commitAsync(null);
  }

  @Override
  synchronized public void commitAsync(OffsetCommitCallback callback) {
    List<ClusterConsumerPair<K, V>> consumers = getImmutableConsumerList();
    AtomicInteger pendingCommitCount = new AtomicInteger(consumers.size());
    Map<TopicPartition, OffsetAndMetadata> allOffsets = new HashMap<>();
    Exception exception = null;
    OffsetCommitCallback wrapperCallback = new WrapperOffsetCommitCallback(pendingCommitCount, allOffsets, exception,
        callback);
    for (ClusterConsumerPair<K, V> entry : getImmutableConsumerList()) {
      entry.getConsumer().commitAsync(wrapperCallback);
    }
  }

  @Override
  synchronized public void commitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, OffsetCommitCallback callback) {
    List<ClusterConsumerPair<K, V>> consumers = getImmutableConsumerList();
    Map<ClusterDescriptor, Map<TopicPartition, OffsetAndMetadata>> perClusterPartitions =
        getPartitionValueMapByCluster(offsets);
    AtomicInteger pendingCommitCount = new AtomicInteger(perClusterPartitions.size());
    Map<TopicPartition, OffsetAndMetadata> allOffsets = new HashMap<>();
    Exception exception = null;
    OffsetCommitCallback wrapperCallback = new WrapperOffsetCommitCallback(pendingCommitCount, allOffsets, exception, callback);
    for (Map.Entry<ClusterDescriptor, Map<TopicPartition, OffsetAndMetadata>> entry : perClusterPartitions.entrySet()) {
      getOrCreatePerClusterConsumer(consumers, entry.getKey()).commitAsync(entry.getValue(), wrapperCallback);
    }
    // ATTN: Need to deal with nonexistent partitions here
  }

  /**
   * Overrides the fetch offsets that the consumer will use on the next {@link #poll(Duration) poll(timeout)}. If this API
   * is invoked for the same partition more than once, the latest offset will be used on the next poll(). Note that
   * you may lose data if this API is arbitrarily used in the middle of consumption, to reset the fetch offsets
   *
   * @throws IllegalArgumentException if the provided offset is negative
   * @throws IllegalStateException if the provided TopicPartition is not assigned to this consumer
   */
  @Override
  synchronized public void seek(TopicPartition partition, long offset) {
    // User assignment - can be checked at the federated consumer level  - ok to seek for nonexistent partition
    //                   as long as it is part of the assignment set.
    // Auto assignment - should be cheked by the per-cluster consumer
    if (_currentSubscription.getSubscriptionType() == FederatedSubscriptionState.SubscriptionType.USER_ASSIGNED) {
      Set<TopicPartition> assignment = ((UserAssigned) _currentSubscription).getAssignment();
      if (!assignment.contains(TopicPartition)) {
        throw new IllegalStateException("No current assignment for partition " + partition);
      }
      Set<String> topicsWaitingToBeCreated = _currentSubscription.getTopicsWaitingToBeCreated();
      if (_currentSubscription.getTopicsWaitingToBeCreated().contains(partition.topic())) {
        // TODO: Vanilla consumer allows to seek for user-assigned partitions that do not exist yet.
      } else {
        getConsumerForTopic(partition.topic()).seek(partition, offset);
      }
    } else {
      // ATTN: How to check subscription? Find a cluster for the topic and let it handle?
      getConsumerForTopic(partition.topic()).seek(partition, offset);
    }
  }

  /**
   * Seek to the first offset for each of the given partitions. This function evaluates lazily, seeking to the
   * first offset in all partitions only when {@link #poll(Duration)} or {@link #position(TopicPartition)} are called.
   * If no partitions are provided, seek to the first offset for all of the currently assigned partitions.
   *
   * @throws IllegalArgumentException if {@code partitions} is {@code null}
   * @throws IllegalStateException if any of the provided partitions are not currently assigned to this consumer
   */
  @Override
  synchronized public void seekToBeginning(Collection<TopicPartition> partitions) {
    if (_currentSubscription.getSubscriptionType() == FederatedSubscriptionState.SubscriptionType.USER_ASSIGNED) {
      Set<TopicPartition> assignment = ((UserAssigned) _currentSubscription).getAssignment();
      Set<String> topicsWaitingToBeCreated = _currentSubscription.getTopicsWaitingToBeCreated();
      for (TopicPartition partition : partitions) {
        if (!assignment.contains(TopicPartition)) {
          throw new IllegalStateException("No current assignment for partition " + partition);
        }
      }

      if (_currentSubscription.getTopicsWaitingToBeCreated().contains(partition.topic())) {
        // TODO: Vanilla consumer allows to seek for user-assigned partitions that do not exist yet.
      } else {
        getConsumerForTopic(partition.topic()).seek(partition, offset);
      }
    } else {
      // ATTN: How to check subscription? Find a cluster for the topic and let it handle?
      getConsumerForTopic(partition.topic()).seek(partition, offset);
    }

    List<ClusterConsumerPair<K, V>> consumers = getImmutableConsumerList();
    for (Map.Entry<ClusterDescriptor, Collection<TopicPartition>> entry : getPartitionsByCluster(partitions).entrySet()) {
      getOrCreatePerClusterConsumer(consumers, entry.getKey()).seekToBeginning(entry.getValue());
    }
  }

  @Override
  synchronized public void seekToEnd(Collection<TopicPartition> partitions) {
    List<ClusterConsumerPair<K, V>> consumers = getImmutableConsumerList();
    for (Map.Entry<ClusterDescriptor, Collection<TopicPartition>> entry : getPartitionsByCluster(partitions).entrySet()) {
      getOrCreatePerClusterConsumer(consumers, entry.getKey()).seekToEnd(entry.getValue());
    }
  }

  @Override
  synchronized public void seekToCommitted(Collection<TopicPartition> partitions) {
    List<ClusterConsumerPair<K, V>> consumers = getImmutableConsumerList();
    for (Map.Entry<ClusterDescriptor, Collection<TopicPartition>> entry : getPartitionsByCluster(partitions).entrySet()) {
      getOrCreatePerClusterConsumer(consumers, entry.getKey()).seekToCommitted(entry.getValue());
    }
  }

  @Override
  synchronized public long position(TopicPartition partition) {
    // In case of user-assigned and the partition belongs to non-existent topic, it can still get the position of
    // that partition.
    // For subscription, partitions belong to nonexistent will throw IllegalStateException
    return getOrCreateConsumerForTopic(partition.topic()).position(partition);
  }

  @Override
  synchronized public long position(TopicPartition partition, Duration timeout) {
    return getOrCreateConsumerForTopic(partition.topic()).position(partition, timeout);
  }

  @Override
  synchronized public OffsetAndMetadata committed(TopicPartition partition) {
    return getOrCreateConsumerForTopic(partition.topic()).committed(partition);
  }

  @Override
  synchronized public OffsetAndMetadata committed(TopicPartition partition, Duration timeout) {
    return getOrCreateConsumerForTopic(partition.topic()).committed(partition, timeout);
  }

  @Override
  synchronized public Long committedSafeOffset(TopicPartition partition) {
    return getOrCreateConsumerForTopic(partition.topic()).committedSafeOffset(partition);
  }

  @Override
  synchronized public Map<MetricName, ? extends Metric> metrics() {
    throw new UnsupportedOperationException("Not implemented yet");
  }

  @Override
  synchronized public List<PartitionInfo> partitionsFor(String topic) {
    return getOrCreateConsumerForTopic(topic).partitionsFor(topic);
  }

  @Override
  synchronized public List<PartitionInfo> partitionsFor(String topic, Duration timeout) {
    return getOrCreateConsumerForTopic(topic).partitionsFor(topic, timeout);
  }

  @Override
  public Map<String, List<PartitionInfo>> listTopics() {
    return listTopics(Duration.ofMillis(_defaultApiTimeoutMs));
  }

  @Override
  synchronized public Map<String, List<PartitionInfo>> listTopics(Duration timeout) {
    ConcurrentHashMap<String, List<PartitionInfo>> aggregate = new ConcurrentHashMap<>();
    executeCallbackWithTimeout("listTopics", null,
        (consumer, ignored, timeoutDuration) -> {
          aggregate.putAll(consumer.listTopics(timeoutDuration));
        }, timeout);
    return Collections.unmodifiableMap(aggregate);
  }

  @Override
  synchronized public Set<TopicPartition> paused() {
    Set<TopicPartition> aggregate = new HashSet<>();
    for (ClusterConsumerPair<K, V> entry : getImmutableConsumerList()) {
      aggregate.addAll(entry.getConsumer().paused());
    }
    return aggregate;
  }

  @Override
  synchronized public void pause(Collection<TopicPartition> partitions) {
    List<ClusterConsumerPair<K, V>> consumers = getImmutableConsumerList();
    for (Map.Entry<ClusterDescriptor, Collection<TopicPartition>> entry : getPartitionsByCluster(partitions).entrySet()) {
      getOrCreatePerClusterConsumer(consumers, entry.getKey()).pause(entry.getValue());
    }
  }

  @Override
  synchronized public void resume(Collection<TopicPartition> partitions) {
    List<ClusterConsumerPair<K, V>> consumers = getImmutableConsumerList();
    for (Map.Entry<ClusterDescriptor, Collection<TopicPartition>> entry : getPartitionsByCluster(partitions).entrySet()) {
      getOrCreatePerClusterConsumer(consumers, entry.getKey()).resume(entry.getValue());
    }
  }

  @Override
  public Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> timestampsToSearch) {
    return offsetsForTimes(timestampsToSearch, Duration.ofMillis(_defaultApiTimeoutMs));
  }

  @Override
  synchronized public Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> timestampsToSearch, Duration timeout) {
    ConcurrentHashMap<TopicPartition, OffsetAndTimestamp> aggregate = new ConcurrentHashMap<>();
    executeCallbackWithTimeout("offsetsForTimes", getPartitionValueMapByCluster(timestampsToSearch),
        (consumer, perClusterTimestampsToSearch, timeoutDuration) -> {
          aggregate.putAll(consumer.offsetsForTimes((Map<TopicPartition, Long>) perClusterTimestampsToSearch,
              timeoutDuration));
        }, timeout);
    return aggregate;
  }

  @Override
  public Map<TopicPartition, Long> beginningOffsets(Collection<TopicPartition> partitions) {
    return beginningOffsets(partitions, Duration.ofMillis(_defaultApiTimeoutMs));
  }

  @Override
  synchronized public Map<TopicPartition, Long> beginningOffsets(Collection<TopicPartition> partitions, Duration timeout) {
    ConcurrentHashMap<TopicPartition, Long> aggregate = new ConcurrentHashMap<>();
    executeCallbackWithTimeout("beginningOffsets", getPartitionsByCluster(partitions),
        (consumer, perClusterPartitions, timeoutDuration) -> {
          aggregate.putAll(consumer.beginningOffsets((Collection<TopicPartition>) perClusterPartitions, timeoutDuration));
        }, timeout);
    return aggregate;
  }

  @Override
  public Map<TopicPartition, Long> endOffsets(Collection<TopicPartition> partitions) {
    return endOffsets(partitions, Duration.ofMillis(_defaultApiTimeoutMs));
  }

  @Override
  synchronized public Map<TopicPartition, Long> endOffsets(Collection<TopicPartition> partitions, Duration timeout) {
    ConcurrentHashMap<TopicPartition, Long> aggregate = new ConcurrentHashMap<>();
    executeCallbackWithTimeout("endOffsets", getPartitionsByCluster(partitions),
        (consumer, perClusterPartitions, timeoutDuration) -> {
          aggregate.putAll(consumer.endOffsets((Collection<TopicPartition>) perClusterPartitions, timeoutDuration));
        }, timeout);
    return aggregate;
  }

  @Override
  synchronized public Long safeOffset(TopicPartition partition, long messageOffset) {
    return getOrCreateConsumerForTopic(partition.topic()).safeOffset(partition, messageOffset);
  }

  @Override
  synchronized public Long safeOffset(TopicPartition partition) {
    return getOrCreateConsumerForTopic(partition.topic()).safeOffset(partition);
  }

  @Override
  synchronized public Map<TopicPartition, Long> safeOffsets() {
    Map<TopicPartition, Long> aggregate = new HashMap<>();
    for (ClusterConsumerPair<K, V> entry : getImmutableConsumerList()) {
      aggregate.putAll(entry.getConsumer().safeOffsets());
    }
    return Collections.unmodifiableMap(aggregate);
  }

  @Override
  public void close() {
    close(CONSUMER_CLOSE_MAX_TIMEOUT);
  }

  @Override
  synchronized public void close(long timeout, TimeUnit timeUnit) {
    close(Duration.ofMillis(timeUnit.toMillis(timeout)));
  }

  @Override
  synchronized public void close(Duration timeout) {
    closeNoLock(timeout.toMillis(), TimeUnit.MILLISECONDS);
    _closed = true;
  }


  private void closeNoLock(Duration timeout) {
    executeCallbackWithTimeout("close", null,
        (consumer, ignored, timeoutDuration) -> consumer.close(timeoutDuration.toMillis(), TimeUnit.MILLISECONDS),
        Duration.of(timeout, LiKafkaClientsUtils.convertTimeUnitToChronoUnit(timeUnit)));
  }

  @Override
  synchronized public void wakeup() {
    for (ClusterConsumerPair<K, V> entry : getImmutableConsumerList()) {
      entry.getConsumer().wakeup();
    }
  }

  // Return an immutable view of the currently created consumers.
  private List<ClusterConsumerPair<K, V>> getImmutableConsumerList() {
    List<ClusterConsumerPair<K, V>> consumers = _consumers;
    return Collections.unmodifiableList(consumers);
  }

  // This is used for testing only
  public FederatedSubscriptionState getCurrentSubscription() {
    return _currentSubscription;
  }

  // Construct a map of partitions keyed by cluster from the given collection of partitions.
  private Map<ClusterDescriptor, Collection<TopicPartition>> getPartitionsByCluster(Collection<TopicPartition> partitions) {
    if (partitions == null) {
      throw new IllegalArgumentException("partitions cannot be null");
    }

    if (partitions.isEmpty()) {
      return Collections.emptyMap();
    }

    Map<TopicPartition, ClusterDescriptor> partitionToClusterMap;
    try {
      partitionToClusterMap =
          _mdsClient.getClustersForTopicPartitions(partitions, _clusterGroup, _mdsRequestTimeoutMs);
    } catch (MetadataServiceClientException e) {
      throw new KafkaException("failed to get clusters for topic partitions " + partitions + ": ", e);
    }

    // Reverse the map so that we can have per-cluster topic partition sets.
    Map<ClusterDescriptor, Collection<TopicPartition>> clusterToPartitionsMap = new HashMap<>();
    Set<TopicPartition> nonexistentPartitions = new HashSet<>();
    for (Map.Entry<TopicPartition, ClusterDescriptor> entry : partitionToClusterMap.entrySet()) {
      TopicPartition topicPartition = entry.getKey();
      ClusterDescriptor cluster = entry.getValue();
      if (cluster == null) {
        nonexistentPartitions.add(topicPartition);
      } else {
        clusterToPartitionsMap.computeIfAbsent(cluster, k -> new HashSet<TopicPartition>()).add(topicPartition);
      }
    }

    if (!nonexistentPartitions.isEmpty()) {
      throw new IllegalStateException("found nonexistent partitions: " + nonexistentPartitions);
    }

    return clusterToPartitionsMap;
  }

  // This contains the result of the location lookup for a set of topic partitions across multiple clusters in a cluster
  // group.
  private class <T> PartitionValueMapByClusterResult {
    private Map<ClusterDescriptor, Map<TopicPartition, T>> _clusterToPartitionValueMap;
    private Set<String> _nonexistentTopics;

    public PartitionValueMapByClusterResult() {
      _clusterToPartitionValueMap = Collections.emptyMap();
      _nonexistentTopics = Collections.emptySet();
    }

    public PartitionsValueMapByClusterResult(Map<ClusterDescriptor, Map<TopicPartition, T>> clusterToPartitionValueMap,
        Set<String> nonexistentTopics) {
      _clusterToPartitionValueMap = clusterToPartitionValueMap;
      _nonexistentTopics = nonexistentTopics;
    }

    public Map<ClusterDescriptor, Map<TopicPartition, T>> getClusterToPartitionValueMap() {
      return _clusterToPartitionValueMap;
    }

    public Set<String> getNonexistentTopics() {
      return _nonexistentTopics;
    }
  }

  // Construct a map of maps by cluster, where the inner map is keyed by partition.
  private <T> PartitionValueMapByClusterResult getPartitionValueMapByCluster(Map<TopicPartition, T> valuesByPartition) {
    if (valuesByPartition == null) {
      throw new IllegalArgumentException("partition map cannot be null");
    }

    if (valuesByPartition.isEmpty()) {
      return new PArtitionValueMapByClusterResult();
    }

    Map<TopicPartition, ClusterDescriptor> partitionToClusterMap;
    try {
      partitionToClusterMap =
          _mdsClient.getClustersForTopicPartitions(valuesByPartition.keySet(), _clusterGroup, _mdsRequestTimeoutMs);
    } catch (MetadataServiceClientException e) {
      throw new KafkaException("failed to get clusters for topic partitions " + valuesByPartition.keySet() + ": ", e);
    }

    // Reverse the map so that we can have per-cluster maps.
    Map<ClusterDescriptor, Map<TopicPartition, T>> mapsByCluster = new HashMap<>();
    Set<String> nonexistentTopics = new HashSet<>();
    for (Map.Entry<TopicPartition, ClusterDescriptor> entry : partitionToClusterMap.entrySet()) {
      TopicPartition topicPartition = entry.getKey();
      ClusterDescriptor cluster = entry.getValue();
      if (cluster == null) {
        nonexistentTopics.add(topicPartition.topic());
      } else {
        mapsByCluster.computeIfAbsent(cluster, k -> new HashMap<TopicPartition, T>()).put(topicPartition,
            valuesByPartition.get(topicPartition));
      }
    }

    return new PartitionValueMapByCluster(mapsByCluster, nonexistentTopics);
  }

  // For each cluster in the perClusterInput key set, concurrently call the given callback with the corresponding
  // consumer and per-cluster input values. If perClusterInput is null, the callback will be called for all existing
  // consumers.
  private <I> void executeCallbackWithTimeout(String methodName,
      Map<ClusterDescriptor, I> perClusterInput, ExecutorCallback callback, Duration timeout) {
    List<ClusterConsumerPair<K, V>> allConsumers = getImmutableConsumerList();
    List<ClusterConsumerPair<K, V>> consumers;
    if (perClusterInput == null) {
      // The callback needs to be executed against all consumers
      consumers = allConsumers;
    } else {
      consumers = new ArrayList<>();
      for (ClusterDescriptor cluster: perClusterInput.keySet()) {
        consumers.add(new ClusterConsumerPair<K, V>(cluster, getOrCreatePerClusterConsumer(allConsumers, cluster)));
      }
    }

    if (consumers.isEmpty()) {
      // Nothing to do
      return;
    }

    LOG.debug("starting {} for {} LiKafkaConsumers for cluster group {} in {} {}...", methodName, consumers.size(),
        _clusterGroup, timeout);

    long startTimeMs = System.currentTimeMillis();
    long deadlineTimeMs = startTimeMs + timeout.toMillis();
    CountDownLatch countDownLatch = new CountDownLatch(consumers.size());
    Set<Thread> threads = new HashSet<>();
    for (ClusterConsumerPair<K, V> entry : consumers) {
      ClusterDescriptor cluster = entry.getCluster();
      LiKafkaConsumer<K, V> consumer = entry.getConsumer();
      Thread t = new Thread(() -> {
        try {
          long remainingTimeMs = deadlineTimeMs - System.currentTimeMillis();
          remainingTimeMs = Math.max(remainingTimeMs, 0);
          callback.execute(consumer, perClusterInput == null ? null : perClusterInput.get(cluster),
              Duration.ofMillis(remainingTimeMs));
        } finally {
          countDownLatch.countDown();
        }
      });
      t.setDaemon(true);
      t.setName("LiKafkaConsumer-" + methodName + "-" + cluster.getName());
      t.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
        public void uncaughtException(Thread t, Throwable e) {
          throw new KafkaException("Thread " + t.getName() + " throws exception", e);
        }
      });
      t.start();
      threads.add(t);
    }

    try {
      if (!countDownLatch.await(deadlineTimeMs - System.currentTimeMillis(), TimeUnit.MILLISECONDS)) {
        LiKafkaClientsUtils.dumpStacksForAllLiveThreads(threads);
        throw new KafkaException("fail to perform " + methodName + " for cluster group " + _clusterGroup + " in " +
            timeout);
      }
    } catch (InterruptedException e) {
      throw new KafkaException("fail to perform " + methodName + " for cluster group " + _clusterGroup, e);
    }

    LOG.info("{}: {} LiKafkaConsumers for cluster group {} complete in {} milliseconds", methodName, consumers.size(),
        _clusterGroup, (System.currentTimeMillis() - startTimeMs));
  }

  public LiKafkaConsumerConfig getCommonConsumerConfigs() {
    return _commonConsumerConfigs;
  }

  public void reloadConfig(Map<String, String> newConfigs, UUID commandId) {
    // Go over each consumer, close them, and update existing configs with newConfigs. Since each per-cluster consumer will be
    // instantiated when the client began consuming from that cluster, we just need to clear the mappings and update the configs.
    // since this method is invoked by marioClient, it should be non-blocking, so we create another thread doing above
    LOG.info("Starting reloadConfig for LiKafkaConsumers in clusterGroup {} with new configs {}", _clusterGroup, newConfigs);
    Thread t = new Thread(() -> {
      recreateConsumers(newConfigs, commandId);
    });
    t.setDaemon(true);
    t.setName("LiKafkaConsumer-reloadConfig-" + _clusterGroup.getName());
    t.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
      public void uncaughtException(Thread t, Throwable e) {
        LOG.error("Thread {} throws exception {}", t.getName(), e);
      }
    });

    t.start();
  }

  // Do synchronization on the method that will be closing the consumers since it will be handled by a different thread
  synchronized void recreateConsumers(Map<String, String> newConfigs, UUID commandId) {
    // TODO: ensureOpen() would throw an exception if consumers are already closed. In this case, we might want to send
    // an error message to notify mario that the config reload command fails due to consumer/producer already closed and
    // let mario decide what to do next (re-send the configs etc).
    ensureOpen();

    // Store the original per-cluster topicPartition assignment
    Map<ClusterDescriptor, Collection<TopicPartition>> perClusterTopicPartitionSet = new HashMap<>();
    for (ClusterConsumerPair<K, V> entry : _consumers) {
      perClusterTopicPartitionSet.put(entry.getCluster(), entry.getConsumer().assignment());
    }

    closeNoLock(RELOAD_CONFIG_EXECUTION_TIMEOUT);

    // save the original configs in case re-create consumers with new configs failed, we need to re-create
    // with the original configs
    Map<String, Object> originalConfigs = _commonConsumerConfigs.originals();

    // update existing configs with newConfigs
    // originals() would return a copy of the internal consumer configs, put in new configs and update existing configs
    Map<String, Object> configMap = _commonConsumerConfigs.originals();
    configMap.putAll(newConfigs);
    _commonConsumerConfigs = new LiKafkaConsumerConfig(configMap);

    // re-create per-cluster consumers with new set of configs
    // if any error occurs, recreate all per-cluster consumers with previous last-known-good configs and
    // TODO : send an error back to Mario
    // _consumers should be filled when reload config happens
    List<ClusterConsumerPair<K, V>> newConsumers = new ArrayList<>();

    try {
      for (ClusterConsumerPair<K, V> entry : _consumers) {
        LiKafkaConsumer<K, V> curConsumer = createPerClusterConsumer(entry.getCluster());
        newConsumers.add(new ClusterConsumerPair<K, V>(entry.getCluster(), curConsumer));
      }

      // replace _consumers with newly created consumers
      _consumers.clear();
      _consumers = newConsumers;
    } catch (Exception e) {
      // if any exception occurs, re-create per-cluster consumers with last-known-good configs
      LOG.error("Failed to recreate per-cluster consumers with new config with exception, restore to previous consumers ", e);

      // recreate failed, restore to previous configured consumers
      newConsumers.clear();
      _commonConsumerConfigs = new LiKafkaConsumerConfig(originalConfigs);

      for (ClusterConsumerPair<K, V> entry : _consumers) {
        LiKafkaConsumer<K, V> curConsumer = createPerClusterConsumer(entry.getCluster());
        newConsumers.add(new ClusterConsumerPair<K, V>(entry.getCluster(), curConsumer));
      }

      _consumers = newConsumers;
    }

    // keep previous partition assignment after config reload
    // TODO: after subscribe() is supported, we need to also keep the original subscription
    for (Map.Entry<ClusterDescriptor, Collection<TopicPartition>> entry : perClusterTopicPartitionSet.entrySet()) {
      getOrCreatePerClusterConsumer(entry.getKey()).assign(entry.getValue());
    }

    // Convert the updated configs from Map<String, Object> to Map<String, String> and send the new config to mario server
    // report reload config execution complete to mario server
    Map<String, String> convertedConfig = new HashMap<>();
    // Send the actually used configs to Mario
    for (Map.Entry<String, Object> entry : _commonConsumerConfigs.originals().entrySet()) {
      convertedConfig.put(entry.getKey(), String.valueOf(entry.getValue()));
    }
    // TODO: Add a flag in RELOAD_CONFIG_RESPONSE message to indicate whether re-creating per-cluster consumers is successful
    _mdsClient.reportCommandExecutionComplete(commandId, convertedConfig, MsgType.RELOAD_CONFIG_RESPONSE);

    // re-register federated client with updated configs
    _mdsClient.reRegisterFederatedClient(newConfigs);

    _numConfigReloads++;

    LOG.info("Successfully recreated LiKafkaConsumers in clusterGroup {} with new configs (diff) {}", _clusterGroup, newConfigs);
  }

  // For testing only, wait for reload config command to finish since it's being executed by a different thread
  void waitForReloadConfigFinish() throws InterruptedException {
    long endWaitTime = System.currentTimeMillis() + Duration.ofMinutes(1).toMillis();
    while (_numConfigReloads == 0 && System.currentTimeMillis() < endWaitTime) {
      TimeUnit.MILLISECONDS.sleep(200);
    }
  }

  // Package private for testing
  LiKafkaConsumer<K, V> getPerClusterConsumer(ClusterDescriptor cluster) {
    if (cluster == null) {
      return null;
    }
    for (ClusterConsumerPair<K, V> entry : getImmutableConsumerList()) {
      if (entry.getCluster().equals(cluster)) {
        return entry.getConsumer();
      }
    }
    return null;
  }

  private LiKafkaConsumer<K, V> getConsumerForTopic(String topic) {
    if (topic == null || topic.isEmpty()) {
      throw new IllegalArgumentException("Topic cannot be null or empty");
    }

    ClusterDescriptor cluster = null;
    try {
      cluster = _mdsClient.getClusterForTopic(topic, _clusterGroup, _mdsRequestTimeoutMs);
    } catch (MetadataServiceClientException e) {
      throw new KafkaException("failed to get cluster for topic " + topic + ": ", e);
    }
    return (cluster == null) ? null : getPerClusterConsumer(cluster);
  }

  private LiKafkaConsumer<K, V> getOrCreateConsumerForTopic(String topic) {
    if (topic == null || topic.isEmpty()) {
      throw new IllegalArgumentException("Topic cannot be null or empty");
    }

    ClusterDescriptor cluster = null;
    try {
      cluster = _mdsClient.getClusterForTopic(topic, _clusterGroup, _mdsRequestTimeoutMs);
    } catch (MetadataServiceClientException e) {
      throw new KafkaException("failed to get cluster for topic " + topic + ": ", e);
    }
    if (cluster == null) {
      throw new IllegalStateException("Topic " + topic + " not found in the metadata service");
    }
    List<ClusterConsumerPair<K, V>> consumers = _consumers;
    return getOrCreatePerClusterConsumer(consumers, cluster);
  }

  // Returns null if the specified topic does not exist in the cluster group.
  private LiKafkaConsumer<K, V> getOrCreatePerClusterConsumer(ClusterDescriptor cluster) {
    if (cluster == null) {
      throw new IllegalArgumentException("Cluster cannot be null");
    }

    for (ClusterConsumerPair<K, V> entry : _consumers) {
      if (entry.getCluster().equals(cluster)) {
        return entry.getConsumer();
      }
    }

    LiKafkaConsumer<K, V> curConsumer = createPerClusterConsumer(cluster);
    // always update the _consumers map to avoid creating the same consumer multiple times when calling
    // this method
    _consumers.add(new ClusterConsumerPair<K, V>(cluster, curConsumer));
    return curConsumer;
  }

  private LiKafkaConsumer<K, V> createPerClusterConsumer(ClusterDescriptor cluster) {
    // Create per-cluster consumer config where the following cluster-specific properties:
    //   - bootstrap.server - the actual bootstrap URL of the physical cluster to connect to
    //   - max.poll.records - the property value set for the federated consumer / the number of clusters in this cluster group
    //                        if the federated consumer property < the number of clusters in the group, set it to 1
    //                        (poll() will make sure that it won't fetch more than the federated consumer property)
    Map<String, Object> configMap = _commonConsumerConfigs.originals();
    configMap.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.getBootstrapUrl());
    configMap.put(ConsumerConfig.CLIENT_ID_CONFIG, _clientIdPrefix + "-" + cluster.getName());

    // TODO: when subscription is supported, max.poll.records for existing consumers may need to be adjusted if a new
    // consumer talks to a new cluster.
    int maxPollRecordsPerCluster = Math.max(_maxPollRecordsForFederatedConsumer / _numClustersToConnectTo, 1);
    configMap.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecordsPerCluster);
    LiKafkaConsumer<K, V> newConsumer = _consumerBuilder.setConsumerConfig(configMap).build();
    return newConsumer;
  }

  private void ensureOpen() {
    if (_closed) {
      throw new IllegalStateException("this federated consumer has already been closed");
    }
  }

  // TODO: check the client registration state and handle accordingly if it is the fallback mode
  private void refreshSubscriptionMetadata() {
    if (_closed || _currentSubscription.getSubscriptionType() == FederatedSubscriptionState.SubscriptionType.NONE ||
        _currentSubscription.getTopicsWaitingToBeCreated().isEmpty()) {
      return;
    }

    boolean topicCreated = false;
    for (String topic : _currentSubscription.getTopicsWaitingToBeCreated()) {
      try {
        if (_mdsClient.getClusterForTopic(topic, _clusterGroup, _mdsRequestTimeoutMs) != null) {
          topicCreated = true;
          break;
        }
      } catch (MetadataServiceClientException e) {
        LOG.warn("cannot get cluster info for topic {} with the following exception. ignoring...", topic, e);
      }
    }

    if (!topicCreated) {
      return;
    }

    // Re-assign/subscribe
    switch (_currentSubscription.getSubscriptionType()) {
      case USER_ASSIGNED:
        assign(((UserAssigned) _currentSubscription).getAssignment());
        // ATTN: keep the offset separately and do seek on them?
        break;

      default:
        LOG.error("unsupported subscription type: {}", _currentSubscription.getSubscriptionType().name());
        throw new IllegalStateException("Unsupportd subscription type: " + _currentSubscription.getSubscriptionType());
    }
  }
}
